# 2026-02-03 — gpt‑oss‑20b (GGUF) on single RTX 3060

**Goal →** verify gpt‑oss‑20b fits and runs fast on a single 12 GB GPU.

**Setup →**
- Model: `ggml-org/gpt-oss-20b-GGUF` (GGUF, default)
- llama.cpp: `llama-server` via `-hf`
- GPU: single RTX 3060 12 GB (use only one GPU for stability)

**Command →**
```bash
CUDA_VISIBLE_DEVICES=0 llama-server \
  -hf ggml-org/gpt-oss-20b-GGUF \
  -c 8192 -b 128 -ub 64 \
  --jinja --host 0.0.0.0 --port 8080
```

**Metrics →**
- Generation: **~66–68 tok/s** (≈2000 tokens in ~30.3s)

**Conclusion →**
- gpt‑oss‑20b GGUF fits and runs **very fast** on a single 12 GB GPU.
