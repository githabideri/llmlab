# Example environment for llama-server
MODEL_PATH=/mnt/models/gguf/qwen3-coder-next/Qwen3-Coder-Next-REAP-40B-A3B-Q2_K_XL.gguf
CUDA_VISIBLE_DEVICES=0,1
CTX_SIZE=131072
BATCH=128
UBATCH=64
TENSOR_SPLIT=14,10
N_CPU_MOE=0
CACHE_TYPE_K=q8_0
CACHE_TYPE_V=q4_0
FLASH_ATTN=1
